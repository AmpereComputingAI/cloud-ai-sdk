# GPT-2  Large

This directory contains scripts to generate and execute models efficiently on AIC100.

GPT-2 Large is a transformer-based language model created and released by OpenAI.
The model is a pretrained model on English language using a causal language modeling (CLM) objective.

## Source of the model

- https://huggingface.co/gpt2-large


## Dependencies and versions

- Python 3.8.16
- Pytorch 2.0.1
- Transformers 4.33.2
- ONNX 1.14.0
- ONNX Runtime 1.15.1
- ONNX Simplifier 0.4.31
- Protobuf 3.20.3

## Create Python virtual environment, activate and install dependencies

    1. python3.8 -m venv ./gpt2large_venv
    2. source ./gpt2large_venv/bin/activate
    3. pip install -r ./requirements.txt

## Model generation

1. Install patched tranformers library:

    - `git clone --branch v4.33.2 --depth 1 https://github.com/huggingface/transformers`
    - `cd transformers`
    - `git apply ../0001-Add-KV-Changes-to-GPT2-Models-CausalMask-Fix.patch`
    - `pip install .`
	 - `cd ..`

2. Run the generation script:

    - `sudo python3 generateONNX.py --model-name gpt2-large --model-class GPT2LMHeadModel --seq-len 128`

Models will be generated in the `<model-name>-kv` subdirectory.

## Model compilation

**Note: To change the seq_len, context_len edit the specializations.json file.**

1. Run the compilation script:

	- `bash compileModel.sh [model name] [mx6 or fp16] [1-14 nsp]`
	- Example: `bash compileModel.sh gpt2-large mx6 14`

This will compile the model and place the generated QPC's in `qpc` subdirectory.

## Model execution

**Note: --prompt-len and --ctx-len values must be same as defined in the specializations.json used at compile time.**

1. Run the runner script:

	`sudo python3 runModel.py --model-name gpt2-large --prompt-len 32 --ctx-len 128 --qpc ./qpc/<model-name>-kv-<prompt-len>pl-<context-len>cl-<num_nsp>c/ --prompt "<prompt>"`

	Example: `sudo python3 runModel.py --model-name gpt2-large --prompt-len 32 --ctx-len 128 --qpc ./qpc/gpt2-large-kv-32pl-128cl-14c/`

This will generate text with the compiled QPC's and finally print the tokens/s generated by the compiled models.

