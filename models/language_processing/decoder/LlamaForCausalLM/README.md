# LlamaForCausalLM 
Inference for models based on `LlamaForCausalLM` architecture can be executed on Cloud AI 100 platforms using the 3 steps - model generation, model compilation and model execution described below. 

## Dependencies and versions

- Python 3.8.16
- Pytorch 2.0.1
- Transformers 4.31.0
- ONNX 1.14.0
- ONNX Runtime 1.15.1
- ONNX Simplifier 0.4.31
- protobuf==3.20.2

## Model generation

1. Install requirements:

    ```
    python3.8 -m venv ./llm_env
    source ./llm_env/bin/activate
    pip install -r requirements.txt
    ```

2. Install patched tranformers library:

    ```
    git clone --branch v4.32.0 --depth 1 https://github.com/huggingface/transformers
    cd transformers
    git apply ../LlamaForCausalLM.patch
    pip install .
    cd ..
    ```

3. Run the generation script:
    - `python generateONNX.py --model-name <Model_Path> --model-class LlamaForCausalLM`
    - Example: `python generateONNX.py --model-name LLM360/Amber --model-class LlamaForCausalLM`

Models will be generated in the `Amber-kv` subdirectory.

## Model compilation

**Note: To change the seq_len, context_len, batch_size, etc. edit the specializations.json file <br> The first section is for prefill and the second is for decode. <br>`batch_size` and `ctx_len` should be identical for a model. <br>`seq_len` aka `prompt_length` may be a power of 2 for the prefill stage and 1 for the decode stage.**

- Run the compilation script: 
    
    `bash compileModel.sh <Path to model generated with generateONNX.py> [mx6 or fp16] [1-16 nsp]`<br>
    Example: `bash compileModel.sh Amber-kv mx6 14`<br>
    Example: `bash compileModel.sh Amber-kv fp16 14`<br>

    **Note: Use '14' (#nsp) for DL2q instance at AWS. '16' (#nsp) can be used at Cirrascale instances**

    This will compile the model and place the generated QPC(s) in `qpc` subdirectory.

### Model support  
  
| # Parameters  | AWS DL2q Instance - 8 Std (14 NSPs) SKU Accelerators| Cirrascale Instance - 1 to 8 Pro (16 NSPs) SKU Accelerators|
| ------------- | ------------- | ----------------- |
| 7B  | FP16 and MX6  | FP16 and MX6 |
| 13B  | MX6  | FP16 and MX6 |

#### Tested Configurations for AWS DL2q Instance 
|# Parameters | Ctx_len  | seq_len aka input-len | Batch-Size | MX6/FP16 | 
| ------ | ------------- | ------------- | ----------------- | -------- | 
|7B | 2048  | 256  | 1 | FP16 | 
|7B | 2048 | 256 | 1 | MX6 | 
| 13B | 2048 | 256 | 1 | MX6 |


## Model execution

- Run the runModel.py script:
    --input-len (aka prompt-len or seq-len) and --ctx-len values must be the same as defined in the specializations.json used at compile time.
  
    `python runModel.py --model-name <Model_Path> --qpc <qpc/model-name-kv-compile_params> --input-len 256 --device_id 2 --prompt <Enter prompt>` <br>
    Example: `python runModel.py --model-name LLM360/Amber --qpc qpc/Amber-kv-256pl-2048cl-14c --input-len 256 --device_id 2 --prompt <Enter text within double-quotes seperated by | for BS greater than 1>` <br>

    This will generate text with the compiled QPC's and finally print the tokens/s generated by the compiled models.

## References 
- [LlamaForCausal execution on Cloud AI 100](https://quic.github.io/cloud-ai-sdk-pages/latest/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/)
    - [Precision - FP16 and MX6](https://quic.github.io/cloud-ai-sdk-pages/latest/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#compile-the-model)
- [Shared Micro-exponents](https://arxiv.org/abs/2302.08007)
