# LlamaForCausalLM 
Inference for models based on `LlamaForCausalLM` architecture can be executed on Cloud AI 100 platforms using the 3 steps - model generation, model compilation and model execution described below. 

## Dependencies and versions

- Python 3.8.16
- Pytorch 2.0.1
- Transformers 4.31.0
- ONNX 1.14.0
- ONNX Runtime 1.15.1
- ONNX Simplifier 0.4.31
- protobuf==3.20.2

## Model generation

1. Install requirements:

    - `python3.8 -m venv ./llm_env`
    - `source ./llm_env/bin/activate`
    - `pip install -r requirements.txt`

2. Install patched tranformers library:

    1. `git clone --branch v4.32.0 --depth 1 https://github.com/huggingface/transformers`
    2. `cd transformers`
    3. `git apply ../LlamaForCausalLM.patch`
    4. `pip install .`
    5. `cd ..`

3. Run the generation script:
    - `python generateONNX.py --model-name <Model_Path> --model-class LlamaForCausalLM`
    - `python generateONNX.py --model-name ./models--LLM360--Amber/snapshots/e691febcfceb864f235225f4e6dcae07e2be6332 --model-class LlamaForCausalLM`

Models will be generated in the `e691febcfceb864f235225f4e6dcae07e2be6332-kv` subdirectory.

## Model compilation

**Note: To change the seq_len, context_len, batch_size, etc. edit the specializations.json file <br> The first section is for prefill and the second is for decode. <br>`batch_size` and `ctx_len` should be identical for a model. <br>`seq_len` aka `prompt_length` may be a power of 2 for the prefill stage and 1 for the decode stage.**

- Run the compilation script: 
    
    `bash compileModel.sh <Path to model generated with generateONNX.py> [mx6 or fp16] [1-16 nsp]`<br>
    `bash compileModel.sh e691febcfceb864f235225f4e6dcae07e2be6332-kv mx6 14`<br>
    `bash compileModel.sh e691febcfceb864f235225f4e6dcae07e2be6332-kv fp16 14`<br>

    **Note: Use '14' (#nsp) for DL2q instance at AWS. '16' (#nsp) can be used at Cirrascale instances**

    This will compile the model and place the generated QPC(s) in `qpc` subdirectory.

## Model execution

- Run the runModel.py script:
    --input-len (aka prompt-len or seq-len) and --ctx-len values must be the same as defined in the specializations.json used at compile time.
  
    `python runModel.py --model-name <Model_Path> --qpc <qpc/model-name-kv-compile_params> --input-len 256 --device_id 2 --prompt <Enter prompt>` <br>
    `python runModel.py --model-name ./models--LLM360--Amber/snapshots/e691febcfceb864f235225f4e6dcae07e2be6332 --qpc qpc/e691febcfceb864f235225f4e6dcae07e2be6332-kv-256pl-2048cl-14c --input-len 256 --device_id 2 --prompt <Enter text within double-quotes seperated by | for BS greater than 1>` <br>

    This will generate text with the compiled QPC's and finally print the tokens/s generated by the compiled models.

## References 
- [LlamaForCausal execution on Cloud AI 100](https://quic.github.io/cloud-ai-sdk-pages/latest/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/)
    - [Precision - FP16 and MX6](https://quic.github.io/cloud-ai-sdk-pages/latest/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#compile-the-model)
- [Shared Micro-exponents](https://arxiv.org/abs/2302.08007)
