# GPTBigCodeForCausalLM 
Inference for models based on `GPTBigCodeForCausalLM` architecture can be executed on Cloud AI 100 platforms using the 3 steps - model generation, model compilation and model execution described below. Some examples of models supported by this recipe are 
```
bigcode/starcoderbase-1b
bigcode/starcoderbase-3b
bigcode/starcoderbase-7b
bigcode/starcoder
bigcode/gpt_bigcode-santacoder
```

## Dependencies and versions

- Python 3.8.16
- Pytorch < 2
- Transformers 4.31.0
- ONNX 1.14.0
- ONNX Runtime 1.15.1
- ONNX Simplifier 0.4.31
- protobuf==3.20.2

## Model generation

1. Install requirements:

    ```
    python3.8 -m venv ./llm_env
    source ./llm_env/bin/activate
    pip install -r requirements.txt
    ```

2. Install patched tranformers library:

    ```
    git clone --branch v4.36.2 --depth 1 https://github.com/huggingface/transformers
    cd transformers
    git apply ../GPTBigCodeForCausalLM.patch
    pip install .
    cd ..
    ```

3. Run the generation script:
    - `python generateONNX.py --model-name <Model_Path> --model-class AutoModelForCausalLM`
    - Example: `python generateONNX.py --model-name bigcode/starcoder --model-class AutoModelForCausalLM`

    Models will be generated in the `starcoder-kv` subdirectory.

## Model compilation

**Note: To change the seq_len, context_len, batch_size, etc. edit the specializations.json file <br> The first section is for prefill and the second is for decode. <br>`batch_size` and `ctx_len` should be identical for prefill and decode. <br>`seq_len` aka `prompt_length` may be a power of 2 for the prefill stage and 1 for the decode stage.**

- Run the compilation script: 
    
    `bash compileModel.sh <Path to model generated with generateONNX.py> [mx6 or fp16] [1-16 nsp]`<br>
    Example: `bash compileModel.sh starcoder-kv mx6 14`<br>
    Example: `bash compileModel.sh starcoder-kv fp16 14`<br>

    **Note: Use '14' (#nsp) for DL2q instance at AWS. '16' (#nsp) can be used at Cirrascale instances**

    This will compile the model and place the generated QPC(s) in `qpc` subdirectory.

    For higher batch size support, compile the model with compileModel_pl1.sh. Set the desired batch_size and ctx_len in the specialization_pl1.json file. Set seq_len = 1.
    bash compileModel_pl1.sh <Path to model generated with generateONNX.py> [mx6 or fp16] [1-16 nsp]
    Example: bash compileModel_pl1.sh starcoder-kv mx6 14
    Example: bash compileModel_pl1.sh starcoder-kv fp16 14

### Model support  
  
#### Tested Configurations (bigcode/starcoder) for AWS DL2q Instance 
|# Parameters | Ctx_len  | seq_len aka input-len | Batch-Size | MX6/FP16 | Comments |
| ------ | ------------- | ------------- | ----------------- | -------- | ------ |
|15B | 8192  | 1  | 14 | mx6 | use compileModel_pl1.sh |
|15B | 2048  | 1  | 56 | mx6 | use compileModel_pl1.sh |
|15B | 1024  | 1  | 84 | mx6 | use compileModel_pl1.sh |

## Model execution

- Run the runModel.py script:
    `cp ../LlamaForCausalLM/qaic_infer.py`
    `cp ../LlamaForCausalLM/runModel.py`  
    `python runModel.py --model-name <Model_Path> --qpc <qpc/model-name-kv-compile_params> --device_id 2 --prompt <Enter prompt> --no-stream` <br>
    Example: `python runModel.py --model-name bigcode/starcoder --qpc qpc/Amber-kv-256pl-2048cl-14c --device_id 2 --prompt <Enter text within double-quotes seperated by | for BS greater than 1> --no-stream` <br>

    This will generate text with the compiled QPC's and finally print the tokens/s generated by the compiled models. <br>
    `runModel.py` supports user prompt lengths greater than the seq_len provided during model compilation. This feature is called input prompt chunking.

## References 
- [LlamaForCausal execution on Cloud AI 100](https://quic.github.io/cloud-ai-sdk-pages/latest/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/)
    - [Precision - FP16 and MX6](https://quic.github.io/cloud-ai-sdk-pages/latest/Getting-Started/Model-Architecture-Support/Large-Language-Models/llm/#compile-the-model)
- [Shared Micro-exponents](https://arxiv.org/abs/2302.08007)
