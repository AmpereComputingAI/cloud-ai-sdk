diff --git a/src/transformers/modeling_attn_mask_utils.py b/src/transformers/modeling_attn_mask_utils.py
index 434b32c..af46805 100755
--- a/src/transformers/modeling_attn_mask_utils.py
+++ b/src/transformers/modeling_attn_mask_utils.py
@@ -32,9 +32,10 @@ class AttentionMaskConverter:
             Optionally, the sliding window masks can be created if `sliding_window` is defined to a positive integer.
     """
 
-    def __init__(self, is_causal: bool, sliding_window: Optional[int] = None):
+    def __init__(self, is_causal: bool, sliding_window: Optional[int] = None, cache_index: Optional[torch.LongTensor] = None):
         self.is_causal = is_causal
         self.sliding_window = sliding_window
+        self.cache_index = cache_index
 
         if self.sliding_window is not None and self.sliding_window <= 0:
             raise ValueError(
@@ -104,6 +105,7 @@ class AttentionMaskConverter:
                 device=attention_mask_2d.device,
                 past_key_values_length=past_key_values_length,
                 sliding_window=self.sliding_window,
+                cache_index=self.cache_index,
             )
         elif self.sliding_window is not None:
             raise NotImplementedError("Sliding window is currently only implemented for causal masking")
@@ -123,20 +125,29 @@ class AttentionMaskConverter:
         device: torch.device,
         past_key_values_length: int = 0,
         sliding_window: Optional[int] = None,
+        cache_index: Optional[torch.LongTensor] = None,
     ):
         """
         Make causal mask used for bi-directional self-attention.
         """
         bsz, tgt_len = input_ids_shape
-        mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)
+        min_val = -32000
+        # mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)
+        mask = torch.full((tgt_len, tgt_len), torch.tensor(min_val, device=device), device=device)
         mask_cond = torch.arange(mask.size(-1), device=device)
         mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)
 
         mask = mask.to(dtype)
 
         if past_key_values_length > 0:
-            mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)
-
+            if cache_index is None:
+                mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)
+            else:
+                query_indices = torch.arange(tgt_len) +  cache_index
+                key_indices = torch.arange(past_key_values_length)
+                mask_cond = key_indices.view(1,-1) > query_indices.view(-1,1)
+                mask = torch.where(mask_cond, torch.tensor(min_val, dtype=dtype), torch.tensor(0.0, dtype=dtype))
+                return mask[None, None, :, :].expand(bsz, 1, tgt_len, past_key_values_length)
         # add lower triangular sliding window mask if necessary
         if sliding_window is not None:
             diagonal = past_key_values_length - sliding_window + 1
@@ -158,7 +169,7 @@ class AttentionMaskConverter:
 
         inverted_mask = 1.0 - expanded_mask
 
-        return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)
+        return inverted_mask.masked_fill(inverted_mask.to(torch.bool), -32000.0)
 
 
 def _prepare_4d_causal_attention_mask(
@@ -167,6 +178,7 @@ def _prepare_4d_causal_attention_mask(
     inputs_embeds: torch.Tensor,
     past_key_values_length: int,
     sliding_window: Optional[int] = None,
+    cache_index: Optional[torch.LongTensor] = None,
 ):
     """
     Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
@@ -184,7 +196,7 @@ def _prepare_4d_causal_attention_mask(
         sliding_window (`int`, *optional*):
             If the model uses windowed attention, a sliding window should be passed.
     """
-    attn_mask_converter = AttentionMaskConverter(is_causal=True, sliding_window=sliding_window)
+    attn_mask_converter = AttentionMaskConverter(is_causal=True, sliding_window=sliding_window, cache_index=cache_index)
 
     key_value_length = input_shape[-1] + past_key_values_length
 
diff --git a/src/transformers/modeling_outputs.py b/src/transformers/modeling_outputs.py
index aceec7a..0a7fd47 100755
--- a/src/transformers/modeling_outputs.py
+++ b/src/transformers/modeling_outputs.py
@@ -157,7 +157,7 @@ class BaseModelOutputWithPast(ModelOutput):
     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
     hidden_states: Optional[Tuple[torch.FloatTensor]] = None
     attentions: Optional[Tuple[torch.FloatTensor]] = None
-
+    attention_mask_RetainedState: Optional[torch.BoolTensor] = None
 
 @dataclass
 class BaseModelOutputWithCrossAttentions(ModelOutput):
@@ -617,7 +617,7 @@ class CausalLMOutputWithPast(ModelOutput):
     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None
     hidden_states: Optional[Tuple[torch.FloatTensor]] = None
     attentions: Optional[Tuple[torch.FloatTensor]] = None
-
+    attention_mask_RetainedState: Optional[torch.BoolTensor] = None
 
 @dataclass
 class CausalLMOutputWithCrossAttentions(ModelOutput):
diff --git a/src/transformers/models/llama/modeling_llama.py b/src/transformers/models/llama/modeling_llama.py
index 703ebf0..efc4530 100644
--- a/src/transformers/models/llama/modeling_llama.py
+++ b/src/transformers/models/llama/modeling_llama.py
@@ -80,13 +80,13 @@ def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int]
 
 
 def _make_causal_mask(
-    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0
+    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0, cache_index: Optional[torch.LongTensor] = None,
 ):
     warnings.warn(
         "Calling `transformers.models.llama.modeling_llama._make_causal_mask` is deprecated and will be removed in v4.37. Use `transformers.models.llama.modeling_llama.AttentionMaskConverter._make_causal_mask"
     )
     return AttentionMaskConverter._make_causal_mask(
-        input_ids_shape=input_ids_shape, dtype=dtype, device=device, past_key_values_length=past_key_values_length
+        input_ids_shape=input_ids_shape, dtype=dtype, device=device, past_key_values_length=past_key_values_length, cache_index=cache_index
     )
 
 
@@ -102,7 +102,9 @@ class LlamaRMSNorm(nn.Module):
     def forward(self, hidden_states):
         input_dtype = hidden_states.dtype
         hidden_states = hidden_states.to(torch.float32)
-        variance = hidden_states.pow(2).mean(-1, keepdim=True)
+        # variance = hidden_states.pow(2).mean(-1, keepdim=True)
+        div_first = hidden_states * torch.rsqrt(torch.tensor(hidden_states.shape[-1], dtype=torch.float32))
+        variance = div_first.pow(2).sum(-1, keepdim=True)
         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
         return self.weight * hidden_states.to(input_dtype)
 
@@ -275,13 +277,14 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:
 class LlamaAttention(nn.Module):
     """Multi-headed attention from 'Attention Is All You Need' paper"""
 
-    def __init__(self, config: LlamaConfig):
+    def __init__(self, config: LlamaConfig, layer_idx: int):
         super().__init__()
         self.config = config
         self.hidden_size = config.hidden_size
         self.num_heads = config.num_attention_heads
         self.head_dim = self.hidden_size // self.num_heads
-        self.num_key_value_heads = config.num_key_value_heads
+        self.layer_idx = layer_idx
+        self.num_key_value_heads = config.num_key_value_heads_per_layer[layer_idx] if 'num_key_value_heads_per_layer' in dir(config) else config.num_key_value_heads
         self.num_key_value_groups = self.num_heads // self.num_key_value_heads
         self.max_position_embeddings = config.max_position_embeddings
         self.rope_theta = config.rope_theta
@@ -334,6 +337,7 @@ class LlamaAttention(nn.Module):
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        cache_index: Optional[torch.LongTensor] = None,
         output_attentions: bool = False,
         use_cache: bool = False,
         **kwargs,
@@ -374,20 +378,39 @@ class LlamaAttention(nn.Module):
         kv_seq_len = key_states.shape[-2]
         if past_key_value is not None:
             kv_seq_len += past_key_value[0].shape[-2]
+            if cache_index is not None:
+                kv_seq_len = past_key_value[0].shape[-2]
         cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)
 
         if past_key_value is not None:
-            # reuse k, v, self_attention
-            key_states = torch.cat([past_key_value[0], key_states], dim=2)
-            value_states = torch.cat([past_key_value[1], value_states], dim=2)
-
+            if cache_index is not None:
+                seq_length = key_states.shape[2]
+                assert value_states.shape[2] == seq_length
+                kv_indices = torch.arange(seq_length) + cache_index
+                past_key_value[0][:,:,kv_indices] = key_states
+                past_key_value[1][:,:,kv_indices] = value_states
+                key_states, value_states = past_key_value
+
+            if cache_index is None:
+                key_states = torch.cat([past_key_value[0], key_states], dim=2)
+                value_states = torch.cat([past_key_value[1], value_states], dim=2)
+                
         past_key_value = (key_states, value_states) if use_cache else None
 
-        key_states = repeat_kv(key_states, self.num_key_value_groups)
-        value_states = repeat_kv(value_states, self.num_key_value_groups)
+        # key_states = repeat_kv(key_states, self.num_key_value_groups)
+        # value_states = repeat_kv(value_states, self.num_key_value_groups)
+
+        # attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
+
+        key_length = key_states.shape[2]
+        
+        query_states = query_states.reshape(bsz*self.num_key_value_heads, q_len * self.num_key_value_groups, self.head_dim)
+        key_states   = key_states.reshape  (bsz*self.num_key_value_heads, key_length, self.head_dim)
+        value_states = value_states.reshape(bsz*self.num_key_value_heads, key_length, self.head_dim)
+        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2)) / math.sqrt(self.head_dim)
+        attn_weights = attn_weights.reshape(bsz, self.num_heads, q_len, key_length)
 
-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
 
         if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
             raise ValueError(
@@ -404,7 +427,11 @@ class LlamaAttention(nn.Module):
 
         # upcast attention to fp32
         attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
-        attn_output = torch.matmul(attn_weights, value_states)
+        # attn_output = torch.matmul(attn_weights, value_states)
+
+        attn_weights = attn_weights.reshape(bsz*self.num_key_value_heads, q_len * self.num_key_value_groups, key_length)
+        attn_output = torch.bmm(attn_weights, value_states)
+        attn_output = attn_output.reshape(bsz, self.num_heads, q_len, self.head_dim)
 
         if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
             raise ValueError(
@@ -623,11 +650,12 @@ class LlamaFlashAttention2(LlamaAttention):
 
 
 class LlamaDecoderLayer(nn.Module):
-    def __init__(self, config: LlamaConfig):
+    def __init__(self, config: LlamaConfig, layer_idx: int):
         super().__init__()
         self.hidden_size = config.hidden_size
+        self.layer_idx = layer_idx
         self.self_attn = (
-            LlamaAttention(config=config)
+            LlamaAttention(config=config, layer_idx=layer_idx)
             if not getattr(config, "_flash_attn_2_enabled", False)
             else LlamaFlashAttention2(config=config)
         )
@@ -641,6 +669,7 @@ class LlamaDecoderLayer(nn.Module):
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        cache_index: Optional[torch.LongTensor] = None,
         output_attentions: Optional[bool] = False,
         use_cache: Optional[bool] = False,
         **kwargs,
@@ -674,6 +703,7 @@ class LlamaDecoderLayer(nn.Module):
             attention_mask=attention_mask,
             position_ids=position_ids,
             past_key_value=past_key_value,
+            cache_index=cache_index,
             output_attentions=output_attentions,
             use_cache=use_cache,
             **kwargs,
@@ -820,7 +850,7 @@ class LlamaModel(LlamaPreTrainedModel):
         self.vocab_size = config.vocab_size
 
         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
-        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
+        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])
         self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)
 
         self.gradient_checkpointing = False
@@ -840,6 +870,7 @@ class LlamaModel(LlamaPreTrainedModel):
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
+        cache_index: Optional[torch.LongTensor] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         use_cache: Optional[bool] = None,
         output_attentions: Optional[bool] = None,
@@ -864,29 +895,64 @@ class LlamaModel(LlamaPreTrainedModel):
         else:
             raise ValueError("You have to specify either input_ids or inputs_embeds")
 
+        seq_length_with_past = seq_length
         past_key_values_length = 0
+
         if past_key_values is not None:
             past_key_values_length = past_key_values[0][0].shape[2]
-
+            seq_length_with_past = seq_length_with_past + past_key_values_length
+            if cache_index is not None:
+                seq_length_with_past = past_key_values_length
+
+        if cache_index is not None:
+            # print("cache_index", cache_index)
+            if position_ids is None:
+                if attention_mask is None:
+                    device = input_ids.device if input_ids is not None else inputs_embeds.device
+                    position_ids = torch.arange(
+                        past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
+                    )
+                    position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
+                else:
+                    # we are always passing attention_mask
+                    # print("attention_mask", attention_mask)
+                    position_ids = (torch.cumsum(attention_mask, 1) - 1) * attention_mask
+                    # print(position_ids)
+                    # print("past_key_values_length", past_key_values_length)
+                    if past_key_values_length > 0:
+                        position_ids = position_ids[:, cache_index].unsqueeze(1)
+                        # print("position ids not getting updated here", position_ids)
+
+        # kv prefill
         if position_ids is None:
-            device = input_ids.device if input_ids is not None else inputs_embeds.device
-            position_ids = torch.arange(
-                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
-            )
-            position_ids = position_ids.unsqueeze(0)
+            if attention_mask is None:
+                device = input_ids.device if input_ids is not None else inputs_embeds.device
+                position_ids = torch.arange(
+                    past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device
+                )
+                position_ids = position_ids.unsqueeze(0).view(-1, seq_length)
+            else:
+                position_ids = (torch.cumsum(attention_mask, 1) - 1) * attention_mask
 
         if inputs_embeds is None:
             inputs_embeds = self.embed_tokens(input_ids)
 
+        if past_key_values_length > 0 and cache_index is not None:
+            attention_mask[:, cache_index + seq_length - 1] = True
+            
+        pre_prepare_attention_mask = attention_mask
+
         if getattr(self.config, "_flash_attn_2_enabled", False):
             # 2d mask is passed through the layers
             attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None
         else:
             # 4d mask is passed through the layers
             attention_mask = _prepare_4d_causal_attention_mask(
-                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length
+                attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length, cache_index=cache_index
             )
 
+        print(attention_mask.min(), attention_mask.max())
+        
         # embed positions
         hidden_states = inputs_embeds
 
@@ -917,6 +983,7 @@ class LlamaModel(LlamaPreTrainedModel):
                     past_key_value,
                     output_attentions,
                     use_cache,
+                    cache_index=cache_index,
                 )
             else:
                 layer_outputs = decoder_layer(
@@ -925,6 +992,7 @@ class LlamaModel(LlamaPreTrainedModel):
                     position_ids=position_ids,
                     past_key_value=past_key_value,
                     output_attentions=output_attentions,
+                    cache_index=cache_index,
                     use_cache=use_cache,
                 )
 
@@ -950,6 +1018,7 @@ class LlamaModel(LlamaPreTrainedModel):
             past_key_values=next_cache,
             hidden_states=all_hidden_states,
             attentions=all_self_attns,
+            attention_mask_RetainedState=pre_prepare_attention_mask if past_key_values is not None else None,            
         )
 
 
@@ -991,6 +1060,7 @@ class LlamaForCausalLM(LlamaPreTrainedModel):
         attention_mask: Optional[torch.Tensor] = None,
         position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
+        cache_index: Optional[torch.LongTensor] = None,        
         inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
@@ -1036,6 +1106,7 @@ class LlamaForCausalLM(LlamaPreTrainedModel):
             attention_mask=attention_mask,
             position_ids=position_ids,
             past_key_values=past_key_values,
+            cache_index=cache_index,            
             inputs_embeds=inputs_embeds,
             use_cache=use_cache,
             output_attentions=output_attentions,
@@ -1049,7 +1120,7 @@ class LlamaForCausalLM(LlamaPreTrainedModel):
             logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
             logits = torch.cat(logits, dim=-1)
         else:
-            logits = self.lm_head(hidden_states)
+            logits = self.lm_head(hidden_states[:,-1,:])
         logits = logits.float()
 
         loss = None
@@ -1075,6 +1146,7 @@ class LlamaForCausalLM(LlamaPreTrainedModel):
             past_key_values=outputs.past_key_values,
             hidden_states=outputs.hidden_states,
             attentions=outputs.attentions,
+            attention_mask_RetainedState=outputs.attention_mask_RetainedState,            
         )
 
     def prepare_inputs_for_generation(
