{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a58acd8",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved. <br>\n",
    "SPDX-License-Identifier: BSD-3-Clause-Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65fc84",
   "metadata": {},
   "source": [
    "**Takeaways:** Users will learn how to onboard a BERT base neural network model on Cloud AI devices and run inference\n",
    "\n",
    "**Before you start:** \n",
    "- There are some commands (folder locations etc) that will need to be updated in this notebook based on the platform and installation location. \n",
    "- The terms 'model' and 'network' are used interchangeably in this notebook. \n",
    "\n",
    "**Last Verified Qualcomm Cloud AI Platform SDK and Apps SDK Version:** Platform SDK 1.10.0.193 and Apps SDK 1.10.0.193 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0c3a7",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "This notebook is for beginners and will take the user through the workflow, from onboarding the 'Bert Base Cased' model (from HuggingFace) to execution of inference on Cloud AI devices. \n",
    "\n",
    "Here is the Cloud AI inference workflow at a high level. \n",
    "\n",
    "\n",
    "![Workflow](Images/Workflow.jpg)\n",
    "\n",
    "\n",
    "We will follow this sequence of steps in the notebook. \n",
    "\n",
    "1. **Install required packages**: Begin by installing all the required packages. We will begin by importing all the necessary libraries and importing all the required dependencies.\n",
    "2. **torch-cpu inference**: Import the model, generate an input and run the model on CPU.\n",
    "3. **ONNX conversion**: We will convert the pytorch model to onnx format. \n",
    "4. **Compilation**: Compile the model for Qualcomm Cloud AI 100.\n",
    "5. **Creating a Session and setting up inputs**: Create a qaic session and prepare the models for qaic runtime.\n",
    "6. **Inference on Cloud AI using Python APIs**: Run inference using qaic api and decode the output.\n",
    "7. **Inference on Cloud AI using qaic-runner CLI**: Run inference with qaic-runner CLI and decode the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c21c6",
   "metadata": {},
   "source": [
    "# 1. Install required packages \n",
    "\n",
    "We will install the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a9888e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/qti-aic/dev/python/qaic-env/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make sure the Python interpreter path is set properly.\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9f9b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Processing /opt/qti-aic/dev/lib/x86_64/qaic-0.0.1-py3-none-any.whl (from -r requirements.txt (line 8))\n",
      "Requirement already satisfied: onnx==1.12.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: optimum in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: numpy==1.23.4 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.23.4)\n",
      "Requirement already satisfied: onnxruntime in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.12.1)\n",
      "Requirement already satisfied: torch===1.11.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.11.0)\n",
      "Requirement already satisfied: pillow==8.3.2 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (8.3.2)\n",
      "Requirement already satisfied: onnxsim in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.4.35)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnx==1.12.0->-r requirements.txt (line 1)) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnx==1.12.0->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (1.10.1)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (4.31.0)\n",
      "Requirement already satisfied: packaging in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (0.19.1)\n",
      "Requirement already satisfied: datasets in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: flatbuffers in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnxruntime->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: rich in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnxsim->-r requirements.txt (line 7)) (12.0.1)\n",
      "Requirement already satisfied: filelock in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers>=4.26.0->transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (0.1.98)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from coloredlogs->optimum->-r requirements.txt (line 2)) (10.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (1.4.3)\n",
      "Requirement already satisfied: xxhash in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (3.8.6)\n",
      "Requirement already satisfied: responses<0.19 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from rich->onnxsim->-r requirements.txt (line 7)) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from rich->onnxsim->-r requirements.txt (line 7)) (2.16.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from sympy->optimum->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from pandas->datasets->optimum->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from pandas->datasets->optimum->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum->-r requirements.txt (line 2)) (1.16.0)\n",
      "qaic is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f94087",
   "metadata": {},
   "source": [
    "## Import the necessary libraries.\n",
    "\n",
    "We will import the pre-trained model from Hugging Face ```transformers``` library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd055e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import sys\n",
    "sys.path.append(\"/opt/qti-aic/examples/apps/qaic-python-sdk\")\n",
    "import qaic\n",
    "import os\n",
    "import torch\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3086d",
   "metadata": {},
   "source": [
    "# 2. Inference using torch-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9aa070",
   "metadata": {},
   "source": [
    "### Choose a model from ```transformers``` library \n",
    "For example: you can provide any pretrained models, but accordingly create ```<model_name>-config.yaml``` file containing compilation and execution options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c57973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = 'distilbert-base-cased-distilled-squad' # Provide a model name supported in transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0187183d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete pre-generated model\n",
    "os.system(f'rm -fr {model_card}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc58e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-trained model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_card)\n",
    "\n",
    "# setup the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bddc1b3-be04-4b9b-8b8f-0b92ba3df321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence example\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "\n",
    "inputs = tokenizer(question, text, return_tensors=\"pt\", max_length=128, truncation=True, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4e8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on CPU\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "784aa6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 14:41:49.693161: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-22 14:41:49.693185: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : a nice puppet\n"
     ]
    }
   ],
   "source": [
    "# process the output\n",
    "start_token_index = outputs.start_logits.argmax()\n",
    "end_token_index = outputs.end_logits.argmax()\n",
    "predict_answer_tokens = inputs.input_ids[0, start_token_index : end_token_index + 1]\n",
    "print(f'Answer : {tokenizer.decode(predict_answer_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276d1f3",
   "metadata": {},
   "source": [
    "# 3. ONNX conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "146713b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dir for saving onnx and qpc files\n",
    "gen_models_path = f\"{model_card}/generatedModels\"\n",
    "os.makedirs(gen_models_path, exist_ok=True)\n",
    "model_base_name = model_card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d03d6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages/torch/onnx/utils.py:1327: UserWarning: Provided key logits for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n",
      "/opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:223: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: ONNX Model is being generated successfully\n"
     ]
    }
   ],
   "source": [
    "# Set dynamic dims and axes.\n",
    "dynamic_dims = {0: 'batch', 1 : 'sequence'}\n",
    "dynamic_axes = {\n",
    "    \"input_ids\" : dynamic_dims,\n",
    "    \"attention_mask\" : dynamic_dims,\n",
    "    \"logits\" : dynamic_dims\n",
    "}\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "inputList = [inputs.input_ids, inputs.attention_mask]\n",
    "\n",
    "model.eval() # setup the model in inference model.\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=tuple(inputList),\n",
    "    f=f\"{gen_models_path}/{model_base_name}.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=input_names,\n",
    "    output_names=['start_logits', 'end_logits'],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=11,\n",
    ")\n",
    "print(\"INFO: ONNX Model is being generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44556144",
   "metadata": {},
   "source": [
    "#### Modification \n",
    "Modify the onnx file to handle ```constants > FP16_Max and < FP16_Min ```. \n",
    "```fix_onnx_fp16``` is a helper function for this purpose. <Br> In the exported model, -inf is represented by the min value in FP32. The helper function modifies that to min in FP16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0812a932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found constants out of FP16 range, clipped to FP16 range\n",
      "Saving modified onnx file at distilbert-base-cased-distilled-squad/generatedModels/distilbert-base-cased-distilled-squad_fix_outofrange_fp16.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnx import numpy_helper\n",
    "        \n",
    "def fix_onnx_fp16(\n",
    "    gen_models_path: str,\n",
    "    model_base_name: str,\n",
    ") -> str:\n",
    "    finfo = np.finfo(np.float16)\n",
    "    fp16_max = finfo.max\n",
    "    fp16_min = finfo.min\n",
    "    model = onnx.load(f\"{gen_models_path}/{model_base_name}.onnx\")\n",
    "    fp16_fix = False\n",
    "    for tensor in onnx.external_data_helper._get_all_tensors(model):\n",
    "        nptensor = numpy_helper.to_array(tensor, gen_models_path)\n",
    "        if nptensor.dtype == np.float32 and (\n",
    "            np.any(nptensor > fp16_max) or np.any(nptensor < fp16_min)\n",
    "        ):\n",
    "            # print(f'tensor value : {nptensor} above {fp16_max} or below {fp16_min}')\n",
    "            nptensor = np.clip(nptensor, fp16_min, fp16_max)\n",
    "            new_tensor = numpy_helper.from_array(nptensor, tensor.name)\n",
    "            tensor.CopyFrom(new_tensor)\n",
    "            fp16_fix = True\n",
    "            \n",
    "    if fp16_fix:\n",
    "        # Save FP16 model\n",
    "        print(\"Found constants out of FP16 range, clipped to FP16 range\")\n",
    "        model_base_name += \"_fix_outofrange_fp16\"\n",
    "        onnx.save(model, f=f\"{gen_models_path}/{model_base_name}.onnx\")\n",
    "        print(f\"Saving modified onnx file at {gen_models_path}/{model_base_name}.onnx\")\n",
    "    return model_base_name\n",
    "\n",
    "fp16_model_name = fix_onnx_fp16(gen_models_path=gen_models_path, model_base_name=model_base_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55acd146",
   "metadata": {},
   "source": [
    "# 4. Compilation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719630b",
   "metadata": {},
   "source": [
    "`qaic-exec` cli tool is used to compile the model for Qualcomm AI Cloud 100. The input to this tool is `onnx` file generated above. The tool produces a QPC (Qualcomm Program Container) binary file in the path defined by `-aic-binary-dir` argument. \n",
    "\n",
    "### Breakdown of key compile parameters.\n",
    "We have compiled the onnx file \n",
    "- with 4 NSP cores\n",
    "- with float 16 precision\n",
    "- defined onnx symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126a0a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ONNX Model from distilbert-base-cased-distilled-squad/generatedModels/distilbert-base-cased-distilled-squad_fix_outofrange_fp16.onnx\n",
      "loading compiler from: /opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\n",
      "Compile started ............... \n",
      "Compiling model with FP16 precision.\n",
      "Generated binary is present at distilbert-base-cased-distilled-squad/generatedModels/distilbert-base-cased-distilled-squad_fix_outofrange_fp16_qpc\n"
     ]
    }
   ],
   "source": [
    "# COMPILE using qaic-exec\n",
    "os.system(f'rm -fr {model_card}/generatedModels/{model_card}_fix_outofrange_fp16_qpc')\n",
    "\n",
    "!/opt/qti-aic/exec/qaic-exec \\\n",
    "-m=distilbert-base-cased-distilled-squad/generatedModels/distilbert-base-cased-distilled-squad_fix_outofrange_fp16.onnx \\\n",
    "-aic-num-cores=4 \\\n",
    "-convert-to-fp16 \\\n",
    "-onnx-define-symbol=batch,1 -onnx-define-symbol=sequence,128 \\\n",
    "-aic-binary-dir=distilbert-base-cased-distilled-squad/generatedModels/distilbert-base-cased-distilled-squad_fix_outofrange_fp16_qpc \\\n",
    "-aic-hw -aic-hw-version=2.0 \\\n",
    "-compile-only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3982f",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "\n",
    "There are three different approaches to invoke the device for inference. \n",
    "\n",
    "1. Utilizing a command line inferface (CLI) command - ```qaic-runner```\n",
    "2. Employing `Python` API (as shown below)\n",
    "3. Leveraging the `C++` api."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f001b",
   "metadata": {},
   "source": [
    "# 5. Creating a Session and setting up inputs\n",
    "\n",
    "Now the we have compiled the model for Qualcomm Cloud AI 100, we can setup a session to run the inference on the device. ```qaic``` library is a set of APIs that provides support for running inference on AIC100 backend. \n",
    "\n",
    "```Session```: Session is the entry point of these APIs. Session is a factory method which user needs to call to create an instance of session with AIC100 backend.\n",
    "\n",
    "### API:\n",
    "```Session(model_qpc_path: str, **kwargs)```\n",
    "\n",
    "\n",
    "### Examples:\n",
    "Creating Session with options passed as KW args\n",
    "```python\n",
    "sess = qaic.Session(model_path='/path/to/qpc', num_activations = 8, set_size=10) \n",
    "```\n",
    " \n",
    "Creating a Session by passing options in yaml file\n",
    "```python\n",
    "sess = qaic.Session(model_path='/path/to/qpc', options_path = ‘/path/xyz/options.yaml’)\n",
    "```\n",
    "\n",
    "\n",
    "### **Limitations**\n",
    "- APIs are compatible with only python 3.8 \n",
    "- These APIs are supported only on x86 platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70722673",
   "metadata": {},
   "source": [
    "Lets create a bert session \n",
    "`distilbert-base-cased-distilled-squad-config.yaml` contains inference parameters like num_activations which is used by qaic.Session \n",
    "along with input data for inference on the Cloud AI device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcd9af45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Inference Parameters\n",
      "num_activations: 2\n",
      "set_size: 10"
     ]
    }
   ],
   "source": [
    "# Contents of our yaml\n",
    "options_path = f'{model_card}-config.yaml'\n",
    "_ = os.system(f'cat {options_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84930902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path of QPC generated with qaic-exec\n",
    "qpcPath = f'{model_card}/generatedModels/{model_card}_fix_outofrange_fp16_qpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb2f4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /opt/qti-aic/dev/lib/x86_64/libQAic.so\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_sess = qaic.Session(model_path= qpcPath+'/programqpc.bin', options_path=options_path)\n",
    "bert_sess.setup() # Loads the network to the device. If setup() is not called, the network gets loaded just before the first inference.\n",
    "# alternatively, you can also provide arguments in the function call.\n",
    "# bert_sess = qaic.Session(model_path= qpcPath+'/programqpc.bin', num_activations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0916b8",
   "metadata": {},
   "source": [
    "Here we are setting `num_activations = 1` and `set_size = 1`.\n",
    "Additionally, you can provide `device_id` as inference parameters. \n",
    "\n",
    "Please find more details about the options [here](https://docs.qualcomm.com/bundle/resource/topics/AIC_Developer_Guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95999b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token shape (1, 128) and type int64\n",
      "Input attention mask shape (1, 128) and type int64\n",
      "start_logits shape (1, 128) and type float32\n",
      "end_logits shape (1, 128) and type float32\n"
     ]
    }
   ],
   "source": [
    "# Here we are reading out all the input and output shapes/types\n",
    "input_shape, input_type = bert_sess.model_input_shape_dict['input_ids']\n",
    "attn_shape, attn_type = bert_sess.model_input_shape_dict['attention_mask']\n",
    "s_output_shape, s_output_type = bert_sess.model_output_shape_dict['start_logits']\n",
    "e_output_shape, e_output_type = bert_sess.model_output_shape_dict['end_logits']\n",
    "print(f'Input token shape {input_shape} and type {input_type}')\n",
    "print(f'Input attention mask shape {attn_shape} and type {attn_type}')\n",
    "print(f'start_logits shape {s_output_shape} and type {s_output_type}')\n",
    "print(f'end_logits shape {e_output_shape} and type {e_output_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9c016",
   "metadata": {},
   "source": [
    "# 6. Inference on Cloud AI using Python APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f37b6a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID 0\n",
      "\tStatus:Ready\n",
      "\tDram Total:33554432 KB\n",
      "\tNsp Free:8\n",
      "QID 1\n",
      "\tStatus:Ready\n",
      "\tDram Total:33554432 KB\n",
      "\tNsp Free:16\n"
     ]
    }
   ],
   "source": [
    "## Check health of the cards before deploying the inference. \n",
    "## Status:Ready indicates that the card is in good health and ready to accept inferences\n",
    "## Status:Error indicates that the card is not in good health. Please contact the system administrator\n",
    "!/opt/qti-aic/tools/qaic-util -q | grep -e \"Status\" -e \"QID\" -e \"Nsp Free\" -e \"Dram Total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf95275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a input dictionary for given input.\n",
    "input_dict = {\"input_ids\": inputs.input_ids.numpy().astype(input_type), \"attention_mask\" : inputs.attention_mask.numpy().astype(attn_type)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d497145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on Qualcomm Cloud AI 100\n",
    "output = bert_sess.run(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e39549a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : a nice puppet\n"
     ]
    }
   ],
   "source": [
    "# Restructure the data from output buffer with output_shape, output_type\n",
    "start_token_index = np.frombuffer(output['start_logits'], dtype=s_output_type).reshape(s_output_shape).argmax()\n",
    "end_token_index = np.frombuffer(output['end_logits'], dtype=e_output_type).reshape(e_output_shape).argmax()\n",
    "\n",
    "# Decode the output.\n",
    "predict_answer_tokens = inputs.input_ids[0, start_token_index : end_token_index + 1]\n",
    "print(f'Answer : {tokenizer.decode(predict_answer_tokens)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a92cf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the session to release the NSP cores\n",
    "bert_sess.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaic-env",
   "language": "python",
   "name": "qaic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
