{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0995b8bc",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Qualcomm Innovation Center, Inc. All rights reserved. <br>\n",
    "SPDX-License-Identifier: BSD-3-Clause-Clear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe90d3c",
   "metadata": {},
   "source": [
    "**Takeaways:** Users will learn how to onboard a BERT base neural network model on Cloud AI devices and run inference\n",
    "\n",
    "**Before you start:** \n",
    "- There are some commands (folder locations etc) that will need to be updated in this notebook based on the platform and installation location. \n",
    "- The terms 'model' and 'network' are used interchangeably in this notebook. \n",
    "\n",
    "**Last Verified Qualcomm Cloud AI Platform SDK and Apps SDK Version:** Platform SDK 1.10.0.193 and Apps SDK 1.10.0.193 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2741f",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "This notebook is for beginners and will take the user through the workflow, from onboarding the 'Bert Base Cased' model (from HuggingFace) to execution of inference on Cloud AI devices. \n",
    "\n",
    "Here is the Cloud AI inference workflow at a high level. \n",
    "\n",
    "\n",
    "![Workflow](Images/Workflow.jpg)\n",
    "\n",
    "\n",
    "We will follow this sequence of steps in the notebook. \n",
    "\n",
    "1. **Install required packages**: Begin by installing all the required packages. We will begin by importing all the necessary libraries and importing all the required dependencies.\n",
    "2. **torch-cpu inference**: Import the model, generate an input and run the model on CPU.\n",
    "3. **ONNX conversion**: We will convert the pytorch model to onnx format. \n",
    "4. **Compilation**: Compile the model for Qualcomm Cloud AI 100.\n",
    "5. **Creating a Session and setting up inputs**: Create a qaic session and prepare the models for qaic runtime.\n",
    "6. **Inference on Cloud AI using Python APIs**: Run inference performantly using qaic api and ThreadPoolExecuter. \n",
    "7. **Inference on Cloud AI using qaic-runner CLI**: Run inference using qaic-runner CLI. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a53f1e",
   "metadata": {},
   "source": [
    "# 1. Install required packages \n",
    "\n",
    "We will install the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05766bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/qti-aic/dev/python/qaic-env/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's make sure the Python interpreter path is set properly.\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0ca631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /opt/qti-aic/dev/lib/x86_64/qaic-0.0.1-py3-none-any.whl (from -r requirements.txt (line 8))\n",
      "Requirement already satisfied: onnx==1.12.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: optimum in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: numpy==1.23.4 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.23.4)\n",
      "Requirement already satisfied: onnxruntime in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.15.1)\n",
      "Requirement already satisfied: torch===1.11.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.11.0)\n",
      "Requirement already satisfied: pillow==8.3.2 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (8.3.2)\n",
      "Requirement already satisfied: onnxsim in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.4.33)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnx==1.12.0->-r requirements.txt (line 1)) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnx==1.12.0->-r requirements.txt (line 1)) (4.7.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (1.10.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (4.32.0)\n",
      "Requirement already satisfied: packaging in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (0.16.4)\n",
      "Requirement already satisfied: datasets in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from optimum->-r requirements.txt (line 2)) (2.7.0)\n",
      "Requirement already satisfied: flatbuffers in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnxruntime->-r requirements.txt (line 4)) (1.12)\n",
      "Requirement already satisfied: rich in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from onnxsim->-r requirements.txt (line 7)) (12.0.1)\n",
      "Requirement already satisfied: filelock in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (3.12.2)\n",
      "Requirement already satisfied: fsspec in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (2023.6.0)\n",
      "Requirement already satisfied: requests in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (4.65.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (0.3.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 2)) (0.1.98)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from coloredlogs->optimum->-r requirements.txt (line 2)) (10.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (1.4.3)\n",
      "Requirement already satisfied: xxhash in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from datasets->optimum->-r requirements.txt (line 2)) (0.18.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from rich->onnxsim->-r requirements.txt (line 7)) (0.9.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from rich->onnxsim->-r requirements.txt (line 7)) (2.16.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from sympy->optimum->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from aiohttp->datasets->optimum->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from requests->huggingface-hub>=0.8.0->optimum->-r requirements.txt (line 2)) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from pandas->datasets->optimum->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from pandas->datasets->optimum->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/qti-aic/dev/python/qaic-env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum->-r requirements.txt (line 2)) (1.16.0)\n",
      "qaic is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19049ff",
   "metadata": {},
   "source": [
    "## Import the necessary libraries.\n",
    "\n",
    "We will import the pre-trained model from Hugging Face ```transformers``` library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4891f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import sys\n",
    "sys.path.append(\"/opt/qti-aic/examples/apps/qaic-python-sdk\")\n",
    "import qaic\n",
    "import os\n",
    "import torch\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import argparse\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f7bca",
   "metadata": {},
   "source": [
    "# 2. Inference using torch-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7eafe",
   "metadata": {},
   "source": [
    "### Choose a model from ```transformers``` library \n",
    "For example: you can provide any pretrained models, but accordingly create ```<model_name>-config.yaml``` file containing compilation and execution options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4a9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = 'bert-base-cased' # Provide a model name supported in transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346568c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete any pre-generated model\n",
    "os.system('rm -fr bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496db697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Import the pre-trained model\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_card)\n",
    "\n",
    "# setup the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5fdb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence example\n",
    "sentences = [\n",
    "\"The [MASK] sat on the mat.\",\n",
    "\"The lion is considered [MASK] of the jungle.\",\n",
    "\"I saw a [MASK] in the park.\",\n",
    "\"The cat is playing in the [MASK].\",\n",
    "\"The dog is [MASK] a cookie.\",\n",
    "\"The cat is drinking a glass of [MASK].\",\n",
    "\"The [MASK] is sleeping in its bed.\",\n",
    "\"The elephant is walking down the [MASK].\",\n",
    "\"That person is talking on the [MASK].\",\n",
    "\"Are you reading a [MASK]?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de06fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_input(sentence, tokenizer):\n",
    "    max_length = 128\n",
    "    encodings = tokenizer(sentence, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    inputIds = encodings[\"input_ids\"]\n",
    "    attentionMask = encodings[\"attention_mask\"]\n",
    "    mask_token_index = torch.where(encodings['input_ids'] == tokenizer.mask_token_id)[1]\n",
    "    return inputIds, attentionMask, mask_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b96fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input example \n",
    "inputIds, attentionMask, mask_token_index = get_example_input(sentences[0],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3dc9925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 10:29:39.860072: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-08-24 10:29:39.860091: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"boy\" sat on the mat.\n",
      "The lion is considered \"protector\" of the jungle.\n",
      "I saw a \"car\" in the park.\n",
      "The cat is playing in the \"garden\".\n",
      "The dog is \"given\" a cookie.\n",
      "The cat is drinking a glass of \"wine\".\n",
      "The \"dog\" is sleeping in its bed.\n",
      "The elephant is walking down the \"street\".\n",
      "That person is talking on the \"phone\".\n",
      "Are you reading a \"book\"?\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        inputIds, attentionMask, mask_token_index = get_example_input(sentence,tokenizer)\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids=inputIds, attention_mask=attentionMask)\n",
    "\n",
    "        token_logits = model_output.logits\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "        word = tokenizer.decode([torch.argmax(mask_token_logits)])\n",
    "        print(sentence.replace(\"[MASK]\", \"\\\"\"+word+\"\\\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760af26",
   "metadata": {},
   "source": [
    "# 3. ONNX conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e29f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dir for saving onnx and qpc files\n",
    "gen_models_path = f\"{model_card}/generatedModels\"\n",
    "os.makedirs(gen_models_path, exist_ok=True)\n",
    "model_base_name = model_card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61bdce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: ONNX Model is being generated successfully\n"
     ]
    }
   ],
   "source": [
    "# Set dynamic dims and axes.\n",
    "dynamic_dims = {0: 'batch', 1 : 'sequence'}\n",
    "dynamic_axes = {\n",
    "    \"input_ids\" : dynamic_dims,\n",
    "    \"attention_mask\" : dynamic_dims,\n",
    "    \"logits\" : dynamic_dims\n",
    "}\n",
    "input_names = [\"input_ids\", \"attention_mask\"]\n",
    "inputList = [inputIds, attentionMask]\n",
    "\n",
    "model.eval() # setup the model in inference model.\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=tuple(inputList),\n",
    "    f=f\"{gen_models_path}/{model_base_name}.onnx\",\n",
    "    verbose=False,\n",
    "    input_names=input_names,\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes=dynamic_axes,\n",
    "    opset_version=11,\n",
    ")\n",
    "print(\"INFO: ONNX Model is being generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca5eae",
   "metadata": {},
   "source": [
    "#### Modification \n",
    "Modify the onnx file to handle ```constants > FP16_Max and < FP16_Min ```. \n",
    "```fix_onnx_fp16``` is a helper function for this purpose. <Br> In the exported model, -inf is represented by the min value in FP32. The helper function modifies that to min in FP16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef1447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found constants out of FP16 range, clipped to FP16 range\n",
      "Saving modified onnx file at bert-base-cased/generatedModels/bert-base-cased_fix_outofrange_fp16.onnx\n"
     ]
    }
   ],
   "source": [
    "from onnx import numpy_helper\n",
    "        \n",
    "def fix_onnx_fp16(\n",
    "    gen_models_path: str,\n",
    "    model_base_name: str,\n",
    ") -> str:\n",
    "    finfo = np.finfo(np.float16)\n",
    "    fp16_max = finfo.max\n",
    "    fp16_min = finfo.min\n",
    "    model = onnx.load(f\"{gen_models_path}/{model_base_name}.onnx\")\n",
    "    fp16_fix = False\n",
    "    for tensor in onnx.external_data_helper._get_all_tensors(model):\n",
    "        nptensor = numpy_helper.to_array(tensor, gen_models_path)\n",
    "        if nptensor.dtype == np.float32 and (\n",
    "            np.any(nptensor > fp16_max) or np.any(nptensor < fp16_min)\n",
    "        ):\n",
    "            # print(f'tensor value : {nptensor} above {fp16_max} or below {fp16_min}')\n",
    "            nptensor = np.clip(nptensor, fp16_min, fp16_max)\n",
    "            new_tensor = numpy_helper.from_array(nptensor, tensor.name)\n",
    "            tensor.CopyFrom(new_tensor)\n",
    "            fp16_fix = True\n",
    "            \n",
    "    if fp16_fix:\n",
    "        # Save FP16 model\n",
    "        print(\"Found constants out of FP16 range, clipped to FP16 range\")\n",
    "        model_base_name += \"_fix_outofrange_fp16\"\n",
    "        onnx.save(model, f=f\"{gen_models_path}/{model_base_name}.onnx\")\n",
    "        print(f\"Saving modified onnx file at {gen_models_path}/{model_base_name}.onnx\")\n",
    "    return model_base_name\n",
    "\n",
    "fp16_model_name = fix_onnx_fp16(gen_models_path=gen_models_path, model_base_name=model_base_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88a496",
   "metadata": {},
   "source": [
    "# 4. Compilation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43084b19",
   "metadata": {},
   "source": [
    "`qaic-exec` cli tool is used to compile the model for Qualcomm AI Cloud 100. The input to this tool is `onnx` file generated above. The tool produces a QPC (Qualcomm Program Container) binary file in the path defined by `-aic-binary-dir` argument. \n",
    "\n",
    "### Breakdown of key compile parameters.\n",
    "We have compiled the onnx file \n",
    "- with 4 NSP cores\n",
    "- with float 16 precision\n",
    "- defined onnx symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3b28d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ONNX Model from bert-base-cased/generatedModels/bert-base-cased_fix_outofrange_fp16.onnx\n",
      "loading compiler from: /opt/qti-aic/dev/lib/x86_64/libQAicCompiler.so\n",
      "Compile started ............... \n",
      "Compiling model with FP16 precision.\n",
      "Generated binary is present at bert-base-cased/generatedModels/bert-base-cased_fix_outofrange_fp16_qpc\n"
     ]
    }
   ],
   "source": [
    "# COMPILE using qaic-exec\n",
    "os.system('rm -fr bert-base-cased/generatedModels/bert-base-cased_fix_outofrange_fp16_qpc')\n",
    "\n",
    "!/opt/qti-aic/exec/qaic-exec \\\n",
    "-m=bert-base-cased/generatedModels/bert-base-cased_fix_outofrange_fp16.onnx \\\n",
    "-aic-num-cores=4 \\\n",
    "-convert-to-fp16 \\\n",
    "-onnx-define-symbol=batch,1 -onnx-define-symbol=sequence,128 \\\n",
    "-aic-binary-dir=bert-base-cased/generatedModels/bert-base-cased_fix_outofrange_fp16_qpc \\\n",
    "-aic-hw -aic-hw-version=2.0 \\\n",
    "-compile-only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fde940",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "\n",
    "There are three different approaches to invoke the device for inference. \n",
    "\n",
    "1. Utilizing a command line inferface (CLI) command - ```qaic-runner```\n",
    "2. Employing `Python` API (as shown below)\n",
    "3. Leveraging the `C++` api."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabcd2b",
   "metadata": {},
   "source": [
    "# 5. Creating a Session and setting up inputs\n",
    "\n",
    "Now the we have compiled the model for Qualcomm Cloud AI 100, we can setup a session to run the inference on the device. ```qaic``` library is a set of APIs that provides support for running inference on AIC100 backend. \n",
    "\n",
    "```Session```: Session is the entry point of these APIs. Session is a factory method which user needs to call to create an instance of session with AIC100 backend.\n",
    "\n",
    "### API:\n",
    "```Session(model_qpc_path: str, **kwargs)```\n",
    "\n",
    "\n",
    "### Examples:\n",
    "Creating Session with options passed as KW args\n",
    "```python\n",
    "sess = qaic.Session(model_path='/path/to/qpc', num_activations = 8, set_size=10) \n",
    "```\n",
    " \n",
    "Creating a Session by passing options in yaml file\n",
    "```python\n",
    "sess = qaic.Session(model_path='/path/to/qpc', options_path = ‘/path/xyz/options.yaml’)\n",
    "```\n",
    "\n",
    "\n",
    "### **Limitations**\n",
    "- APIs are compatible with only python 3.8 \n",
    "- These APIs are supported only on x86 platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909c9a0",
   "metadata": {},
   "source": [
    "Lets create a bert session \n",
    "`bert-base-cased-config.yaml` contains inference parameters like num_activations which is used by qaic.Session \n",
    "along with input data for inference on the Cloud AI device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64437150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Inference Parameters\n",
      "num_activations: 2\n",
      "set_size: 10"
     ]
    }
   ],
   "source": [
    "# Contents of our yaml\n",
    "options_path = f'{model_card}-config.yaml'\n",
    "_ = os.system(f'cat {options_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "879cffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path of QPC generated with qaic-exec\n",
    "qpcPath = 'bert-base-cased/generatedModels/bert-base-cased_fix_outofrange_fp16_qpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98ff74c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /opt/qti-aic/dev/lib/x86_64/libQAic.so\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_sess = qaic.Session(model_path= qpcPath+'/programqpc.bin', options_path=options_path)\n",
    "bert_sess.setup() # Loads the network to the device. If setup() is not called, the network gets loaded just before the first inference.\n",
    "# alternatively, you can also provide arguments in the function call.\n",
    "# bert_sess = qaic.Session(model_path= qpcPath+'/programqpc.bin', num_activations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9b6d9",
   "metadata": {},
   "source": [
    "Here we are setting `num_activations = 1` and `set_size = 1`.\n",
    "Additionally, you can provide `device_id` as inference parameters. \n",
    "\n",
    "Please find more details about the options [here](https://docs.qualcomm.com/bundle/resource/topics/AIC_Developer_Guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "454eff55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token shape (1, 128) and type int64\n",
      "Input attention mask shape (1, 128) and type int64\n",
      "Output logits shape (1, 128, 28996) and type float32\n"
     ]
    }
   ],
   "source": [
    "# Here we are reading out all the input and output shapes/types\n",
    "input_shape, input_type = bert_sess.model_input_shape_dict['input_ids']\n",
    "attn_shape, attn_type = bert_sess.model_input_shape_dict['attention_mask']\n",
    "output_shape, output_type = bert_sess.model_output_shape_dict['logits']\n",
    "print(f'Input token shape {input_shape} and type {input_type}')\n",
    "print(f'Input attention mask shape {attn_shape} and type {attn_type}')\n",
    "print(f'Output logits shape {output_shape} and type {output_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109988ec",
   "metadata": {},
   "source": [
    "# 6. Inference on Cloud AI using Python APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b2abb8",
   "metadata": {},
   "source": [
    "### `set_size`\n",
    "This helps in managing parallelism on host. When pre/post processing is happening on host, `set_size` helps in running pre/post processing in parallel on host before submitting it to the device.\n",
    "`set_size` value is number of pre/post processing that can happen on host in parallel.\n",
    "\n",
    "### `num_activations`\n",
    "Instances of network loaded onto the device. This helps to run multiple instances of same network in parallel on the device.\n",
    "\n",
    "### worker threads (in `ThreadPoolExecutor`)\n",
    "Since `session.run` is a blocking call, we need threading to submit inference if we want parallelism.\n",
    "These threads are only responsible for submitting inference request to runtime in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e79933",
   "metadata": {},
   "source": [
    "#### General guidance on number of threads\n",
    "\n",
    "By default, the number of threads should be 10. When user is trying to play with `num_activations` and `set_size`, then threads should be `num_activations`\\*`set_size` to get good performance.\n",
    "\n",
    "That being said, number of threads should not be a very high number that process spends too much time in creating threads.\n",
    "\n",
    "Our guidance would be to keep threads as `num_activations`*`set_size` but user needs to monitor how much time is being taken in spawning threads and thread switching. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf4a77",
   "metadata": {},
   "source": [
    "### helper functions\n",
    "\n",
    "`buildinput` : Function to generate input_data for given sentence.\n",
    "\n",
    "`infer` : Runs inference using `sess.run` and post-process to find the masked_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b07ee303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1 : The lion is considered \"protector\" of the jungle.\n",
      "processed 0 : The \"boy\" sat on the mat.\n",
      "processed 2 : I saw a \"car\" in the park.\n",
      "processed 3 : The cat is playing in the \"garden\".\n",
      "processed 4 : The dog is \"given\" a cookie.\n",
      "processed 5 : The cat is drinking a glass of \"wine\".\n",
      "processed 7 : The elephant is walking down the \"street\".\n",
      "processed 6 : The \"dog\" is sleeping in its bed.\n",
      "processed 8 : That person is talking on the \"phone\".\n",
      "processed 9 : Are you reading a \"book\"?\n"
     ]
    }
   ],
   "source": [
    "# Run the model on Qualcomm Cloud AI 100\n",
    "import concurrent.futures\n",
    "\n",
    "def buildinput(sentence):\n",
    "    inputIds, attentionMask, mask_token_index = get_example_input(sentence,tokenizer)\n",
    "    input_dict = {\"input_ids\": inputIds.numpy().astype(input_type), \"attention_mask\" : attentionMask.numpy().astype(attn_type)}\n",
    "    inputs = {'dict' : input_dict, 'mask_token' : mask_token_index}\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def infer(input_data, input_index):\n",
    "    output = bert_sess.run(input_data['dict'])\n",
    "    mask_token_index = input_data['mask_token']\n",
    "    token_logits = np.frombuffer(output['logits'], dtype=output_type).reshape(output_shape) \n",
    "    masked_word = tokenizer.decode([np.argmax(token_logits[0, mask_token_index, :])])\n",
    "    return masked_word, input_index\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    my_input_data = [buildinput(sentence) for sentence in sentences]\n",
    "    futures = [executor.submit(infer, input_data, i) for i, input_data in enumerate(my_input_data)]\n",
    "\n",
    "    for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "        masked_word, input_index = future.result()\n",
    "        print(f'processed {input_index} :',sentences[input_index].replace(\"[MASK]\", \"\\\"\"+masked_word+\"\\\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e724a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the session to release the NSP cores\n",
    "bert_sess.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13c68c",
   "metadata": {},
   "source": [
    "# 7. Inference using qaic-runner CLI tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "983c8981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings saved at: bert-base-cased/inputFiles/0\n",
      "Encodings saved at: bert-base-cased/inputFiles/1\n",
      "Encodings saved at: bert-base-cased/inputFiles/2\n",
      "Encodings saved at: bert-base-cased/inputFiles/3\n",
      "Encodings saved at: bert-base-cased/inputFiles/4\n",
      "Encodings saved at: bert-base-cased/inputFiles/5\n",
      "Encodings saved at: bert-base-cased/inputFiles/6\n",
      "Encodings saved at: bert-base-cased/inputFiles/7\n",
      "Encodings saved at: bert-base-cased/inputFiles/8\n",
      "Encodings saved at: bert-base-cased/inputFiles/9\n"
     ]
    }
   ],
   "source": [
    "# store the example input in files\n",
    "import os\n",
    "\n",
    "def save_encodings(inputIds, attentionMask, i, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    inputIds.detach().numpy().tofile(os.path.join(path, 'input_ids.raw'))\n",
    "    attentionMask.detach().numpy().tofile(os.path.join(path, 'input_mask.raw'))\n",
    "    print('Encodings saved at:', path)\n",
    "    with open(\"inputs_list.txt\", \"a\") as myfile:\n",
    "        myfile.write(f'{path}/input_ids.raw,{path}/input_mask.raw\\n')\n",
    "\n",
    "# get example input\n",
    "for i,sentence in enumerate(sentences):\n",
    "    inputIds, attentionMask, mask_token_index = get_example_input(sentence, tokenizer)\n",
    "    save_encodings(inputIds, attentionMask, i, path=f'bert-base-cased/inputFiles/{i}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6443d5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /opt/qti-aic/dev/lib/x86_64/libQAic.so\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-0.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-1.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-2.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-3.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-4.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-5.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-6.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-7.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-8.bin\n",
      "Writing file:bert-base-cased/outputFiles/logits-activation-0-inf-9.bin\n",
      "Number of Files(as per batch input):10\n",
      " ---- Stats ----\n",
      "InferenceCnt 100 TotalDuration 389731us BatchSize 1 Inf/Sec 256.587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = f'/opt/qti-aic/exec/qaic-runner \\\n",
    "    -t {qpcPath} \\\n",
    "    -a 1 \\\n",
    "    -n 100\\\n",
    "    --aic-batch-input-file-list inputs_list.txt\\\n",
    "    --write-output-start-iter 0\\\n",
    "    --write-output-num-samples 10\\\n",
    "    --write-output-dir bert-base-cased/outputFiles\\\n",
    "    -d 0'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cce57d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input index 0 : The \"boy\" sat on the mat.\n",
      "Input index 1 : The lion is considered \"protector\" of the jungle.\n",
      "Input index 2 : I saw a \"car\" in the park.\n",
      "Input index 3 : The cat is playing in the \"garden\".\n",
      "Input index 4 : The dog is \"given\" a cookie.\n",
      "Input index 5 : The cat is drinking a glass of \"wine\".\n",
      "Input index 6 : The \"dog\" is sleeping in its bed.\n",
      "Input index 7 : The elephant is walking down the \"street\".\n",
      "Input index 8 : That person is talking on the \"phone\".\n",
      "Input index 9 : Are you reading a \"book\"?\n"
     ]
    }
   ],
   "source": [
    "output_shape = (1, 128, 28996)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    inputIds, attentionMask, mask_token_index = get_example_input(sentence, tokenizer)\n",
    "    token_logits = np.fromfile(f'bert-base-cased/outputFiles/logits-activation-0-inf-{i}.bin', output_type).reshape(output_shape)\n",
    "    # Decode the output.\n",
    "    masked_word = tokenizer.decode([np.argmax(token_logits[0, mask_token_index, :])])\n",
    "    print(f'Input index {i} :',sentences[i].replace(\"[MASK]\", \"\\\"\"+masked_word+\"\\\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaic-env",
   "language": "python",
   "name": "qaic-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
